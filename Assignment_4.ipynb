{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMjNEvEplCxlF3q6when6IR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Karan1106/Assignment3/blob/master/Assignment_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXybjfeBDF2t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "2385e730-c1a8-431b-ab32-81fb59a96eee"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaFT9pn6FWJU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7520e062-cf02-486c-ea19-16242dda50c7"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCsvqEwbFX_p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "import os\n",
        "import random\n",
        "from scipy import ndarray"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iv8LpX6nFtyz",
        "colab_type": "text"
      },
      "source": [
        "Data Augmentation using opencv (Jeans)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eN6JRtfyFZlu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy import ndimage, misc\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "def rotate():\n",
        "    outPath = \"/content/gdrive/My Drive/Colab Notebooks/A4 Augmented images/Jeans\"\n",
        "    path = \"/content/gdrive/My Drive/Colab Notebooks/Scrap_train/jeans\"\n",
        "\n",
        "    # iterate through the names of contents of the folder\n",
        "    for image_path in os.listdir(path):\n",
        "\n",
        "        # create the full input path and read the file\n",
        "        input_path = os.path.join(path, image_path)\n",
        "        image_to_rotate = cv2.imread(input_path)\n",
        "\n",
        "        # rotate the image\n",
        "        img_rotate_90_clockwise = cv2.rotate(image_to_rotate, cv2.ROTATE_90_CLOCKWISE)\n",
        "\n",
        "\n",
        "        # create full output path, 'example.jpg' \n",
        "        # becomes 'rotate_example.jpg', save the file to disk\n",
        "        fullpath = os.path.join(outPath, 'rotated_'+image_path)\n",
        "        cv2.imwrite(fullpath, img_rotate_90_clockwise)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    rotate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZP6RFbRjGmj4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resize():\n",
        "    outPath = \"/content/gdrive/My Drive/Colab Notebooks/A4 Augmented images/Jeans\"\n",
        "    path = \"/content/gdrive/My Drive/Colab Notebooks/Scrap_train/jeans\"\n",
        "\n",
        "    # iterate through the names of contents of the folder\n",
        "    for image_path in os.listdir(path):\n",
        "\n",
        "        # create the full input path and read the file\n",
        "        input_path = os.path.join(path, image_path)\n",
        "        image_to_resize = cv2.imread(input_path)\n",
        "\n",
        "        # resize the image\n",
        "        image_resized = cv2.resize(image_to_resize, (300, 200))\n",
        "\n",
        "        image_rgb = cv2.cvtColor(image_to_resize, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # create full output path, 'example.jpg' \n",
        "        # becomes 'resize_example.jpg', save the file to disk\n",
        "        fullpath = os.path.join(outPath, 'resize_'+image_path)\n",
        "        cv2.imwrite(fullpath, image_resized)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    resize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NBVYke0HNGP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def flip():\n",
        "    outPath = \"/content/gdrive/My Drive/Colab Notebooks/A4 Augmented images/Jeans\"\n",
        "    path = \"/content/gdrive/My Drive/Colab Notebooks/Scrap_train/jeans\"\n",
        "\n",
        "    # iterate through the names of contents of the folder\n",
        "    for image_path in os.listdir(path):\n",
        "\n",
        "        # create the full input path and read the file\n",
        "        input_path = os.path.join(path, image_path)\n",
        "        image_to_flip = cv2.imread(input_path)\n",
        "\n",
        "        # flip the image\n",
        "        img_flip_ud = cv2.flip(image_to_flip, 0)\n",
        "        \n",
        "\n",
        "\n",
        "        # create full output path, 'example.jpg' \n",
        "        # becomes 'flip_example.jpg', save the file to disk\n",
        "        fullpath = os.path.join(outPath, 'flip_'+image_path)\n",
        "        cv2.imwrite(fullpath, img_flip_ud)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    flip()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joPrOPW1Hdzh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sharpen():\n",
        "    outPath = \"/content/gdrive/My Drive/Colab Notebooks/A4 Augmented images/Jeans\"\n",
        "    path = \"/content/gdrive/My Drive/Colab Notebooks/Scrap_train/jeans\"\n",
        "\n",
        "    # iterate through the names of contents of the folder\n",
        "    for image_path in os.listdir(path):\n",
        "\n",
        "        # create the full input path and read the file\n",
        "        input_path = os.path.join(path, image_path)\n",
        "        image_to_sharp = cv2.imread(input_path)\n",
        "\n",
        "        # sharp the image\n",
        "        kernel = np.array([[0, -1, 0], \n",
        "                   [-1, 5,-1], \n",
        "                   [0, -1, 0]])\n",
        "        \n",
        "\n",
        "        # Sharpen image\n",
        "        image_sharp = cv2.filter2D(image_to_sharp, -1, kernel)\n",
        "        \n",
        "        # create full output path, 'example.jpg' \n",
        "        # becomes 'sharp_example.jpg', save the file to disk\n",
        "        fullpath = os.path.join(outPath, 'sharp_'+image_path)\n",
        "        cv2.imwrite(fullpath, image_sharp)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    sharpen()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSFF1q4BHglu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bright():\n",
        "    outPath = \"/content/gdrive/My Drive/Colab Notebooks/A4 Augmented images/Jeans\"\n",
        "    path = \"/content/gdrive/My Drive/Colab Notebooks/Scrap_train/jeans\"\n",
        "\n",
        "    # iterate through the names of contents of the folder\n",
        "    for image_path in os.listdir(path):\n",
        "\n",
        "        # create the full input path and read the file\n",
        "        input_path = os.path.join(path, image_path)\n",
        "        image_to_bright = cv2.imread(input_path)\n",
        "\n",
        "        # bright the image\n",
        "        image= cv2.add(image_to_bright,np.array([50.0]))\n",
        "        \n",
        "\n",
        "\n",
        "        # create full output path, 'example.jpg' \n",
        "        # becomes 'bright_example.jpg', save the file to disk\n",
        "        fullpath = os.path.join(outPath, 'bright_'+image_path)\n",
        "        cv2.imwrite(fullpath, image)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    bright()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCDhBwpGHz1g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#decreasing brightness\n",
        "def decbright():\n",
        "    outPath = \"/content/gdrive/My Drive/Colab Notebooks/A4 Augmented images/Jeans\"\n",
        "    path = \"/content/gdrive/My Drive/Colab Notebooks/Scrap_train/jeans\"\n",
        "\n",
        "    # iterate through the names of contents of the folder\n",
        "    for image_path in os.listdir(path):\n",
        "\n",
        "        # create the full input path and read the file\n",
        "        input_path = os.path.join(path, image_path)\n",
        "        image_to_bright = cv2.imread(input_path)\n",
        "\n",
        "        # bright the image\n",
        "        image= cv2.add(image_to_bright,np.array([-50.0]))\n",
        "        \n",
        "\n",
        "\n",
        "        # create full output path, 'example.jpg' \n",
        "        # becomes 'bright_example.jpg', save the file to disk\n",
        "        fullpath = os.path.join(outPath, 'decbright_'+image_path)\n",
        "        cv2.imwrite(fullpath, image)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    decbright()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnuZsW0tMOvE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rotate():\n",
        "    outPath = \"/content/gdrive/My Drive/Colab Notebooks/A4 Augmented images/Jeans\"\n",
        "    path = \"/content/gdrive/My Drive/Colab Notebooks/Scrap_train/jeans\"\n",
        "\n",
        "    # iterate through the names of contents of the folder\n",
        "    for image_path in os.listdir(path):\n",
        "\n",
        "        # create the full input path and read the file\n",
        "        input_path = os.path.join(path, image_path)\n",
        "        image_to_rotate = cv2.imread(input_path)\n",
        "\n",
        "        # rotate the image\n",
        "        height, width= image_to_rotate.shape[:2]\n",
        "        rotation_matrix=cv2.getRotationMatrix2D((width/2, height/2), -30, 0.5)\n",
        "        rotated=cv2.warpAffine(image_to_rotate, rotation_matrix, (width, height))\n",
        "\n",
        "        fullpath = os.path.join(outPath, 'rotated_'+image_path)\n",
        "        cv2.imwrite(fullpath, rotated)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    rotate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60-zXZOIMb1K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rotate():\n",
        "    outPath = \"/content/gdrive/My Drive/Colab Notebooks/A4 Augmented images/Jeans\"\n",
        "    path = \"/content/gdrive/My Drive/Colab Notebooks/Scrap_train/jeans\"\n",
        "\n",
        "    # iterate through the names of contents of the folder\n",
        "    for image_path in os.listdir(path):\n",
        "\n",
        "        # create the full input path and read the file\n",
        "        input_path = os.path.join(path, image_path)\n",
        "        image_to_rotate = cv2.imread(input_path)\n",
        "\n",
        "        # rotate the image\n",
        "        height, width= image_to_rotate.shape[:2]\n",
        "        rotation_matrix=cv2.getRotationMatrix2D((width/2, height/2), 30, 0.5)\n",
        "        rotated=cv2.warpAffine(image_to_rotate, rotation_matrix, (width, height))\n",
        "\n",
        "        fullpath = os.path.join(outPath, 'rotated_'+image_path)\n",
        "        cv2.imwrite(fullpath, rotated)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    rotate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5D0P4eXbH9Gk",
        "colab_type": "text"
      },
      "source": [
        "Data Augmentation using opencv (sarees)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOzVixm9H4pG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rotate():\n",
        "    outPath = \"/content/gdrive/My Drive/Colab Notebooks/A4 Augmented images/Saree\"\n",
        "    path = \"/content/gdrive/My Drive/Colab Notebooks/Scrap_train/saree\"\n",
        "\n",
        "    # iterate through the names of contents of the folder\n",
        "    for image_path in os.listdir(path):\n",
        "\n",
        "        # create the full input path and read the file\n",
        "        input_path = os.path.join(path, image_path)\n",
        "        image_to_rotate = cv2.imread(input_path)\n",
        "\n",
        "        # rotate the image\n",
        "        img_rotate_90_clockwise = cv2.rotate(image_to_rotate, cv2.ROTATE_90_CLOCKWISE)\n",
        "\n",
        "\n",
        "        # create full output path, 'example.jpg' \n",
        "        # becomes 'rotate_example.jpg', save the file to disk\n",
        "        fullpath = os.path.join(outPath, 'rotated_'+image_path)\n",
        "        cv2.imwrite(fullpath, img_rotate_90_clockwise)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    rotate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4huurtrI-40",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bright():\n",
        "    outPath = \"/content/gdrive/My Drive/Colab Notebooks/A4 Augmented images/Saree\"\n",
        "    path = \"/content/gdrive/My Drive/Colab Notebooks/Scrap_train/saree\"\n",
        "\n",
        "    # iterate through the names of contents of the folder\n",
        "    for image_path in os.listdir(path):\n",
        "\n",
        "        # create the full input path and read the file\n",
        "        input_path = os.path.join(path, image_path)\n",
        "        image_to_bright = cv2.imread(input_path)\n",
        "\n",
        "        # bright the image\n",
        "        image= cv2.add(image_to_bright,np.array([50.0]))\n",
        "        \n",
        "\n",
        "\n",
        "        # create full output path, 'example.jpg' \n",
        "        # becomes 'bright_example.jpg', save the file to disk\n",
        "        fullpath = os.path.join(outPath, 'bright_'+image_path)\n",
        "        cv2.imwrite(fullpath, image)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    bright()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfeDaiAhJLre",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def decbright():\n",
        "    outPath = \"/content/gdrive/My Drive/Colab Notebooks/A4 Augmented images/Saree\"\n",
        "    path = \"/content/gdrive/My Drive/Colab Notebooks/Scrap_train/saree\"\n",
        "\n",
        "    # iterate through the names of contents of the folder\n",
        "    for image_path in os.listdir(path):\n",
        "\n",
        "        # create the full input path and read the file\n",
        "        input_path = os.path.join(path, image_path)\n",
        "        image_to_bright = cv2.imread(input_path)\n",
        "\n",
        "        # bright the image\n",
        "        image= cv2.add(image_to_bright,np.array([-50.0]))\n",
        "        \n",
        "\n",
        "\n",
        "        # create full output path, 'example.jpg' \n",
        "        # becomes 'bright_example.jpg', save the file to disk\n",
        "        fullpath = os.path.join(outPath, 'decbright_'+image_path)\n",
        "        cv2.imwrite(fullpath, image)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    decbright()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f80DnlvjJQEj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def flip():\n",
        "    outPath = \"/content/gdrive/My Drive/Colab Notebooks/A4 Augmented images/Saree\"\n",
        "    path = \"/content/gdrive/My Drive/Colab Notebooks/Scrap_train/saree\"\n",
        "\n",
        "    # iterate through the names of contents of the folder\n",
        "    for image_path in os.listdir(path):\n",
        "\n",
        "        # create the full input path and read the file\n",
        "        input_path = os.path.join(path, image_path)\n",
        "        image_to_flip = cv2.imread(input_path)\n",
        "\n",
        "        # flip the image\n",
        "        img_flip_ud = cv2.flip(image_to_flip, 0)\n",
        "        \n",
        "\n",
        "\n",
        "        # create full output path, 'example.jpg' \n",
        "        # becomes 'flip_example.jpg', save the file to disk\n",
        "        fullpath = os.path.join(outPath, 'flip_'+image_path)\n",
        "        cv2.imwrite(fullpath, img_flip_ud)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    flip()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_7yBQ9nJWyB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def resize():\n",
        "    outPath = \"/content/gdrive/My Drive/Colab Notebooks/A4 Augmented images/Saree\"\n",
        "    path = \"/content/gdrive/My Drive/Colab Notebooks/Scrap_train/saree\"\n",
        "\n",
        "    # iterate through the names of contents of the folder\n",
        "    for image_path in os.listdir(path):\n",
        "\n",
        "        # create the full input path and read the file\n",
        "        input_path = os.path.join(path, image_path)\n",
        "        image_to_resize = cv2.imread(input_path)\n",
        "\n",
        "        # resize the image\n",
        "        image_resized = cv2.resize(image_to_resize, (300, 200))\n",
        "\n",
        "        image_rgb = cv2.cvtColor(image_to_resize, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # create full output path, 'example.jpg' \n",
        "        # becomes 'resize_example.jpg', save the file to disk\n",
        "        fullpath = os.path.join(outPath, 'resize_'+image_path)\n",
        "        cv2.imwrite(fullpath, image_resized)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    resize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yq6twK0oJbqw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sharpen():\n",
        "    outPath = \"/content/gdrive/My Drive/Colab Notebooks/A4 Augmented images/Saree\"\n",
        "    path = \"/content/gdrive/My Drive/Colab Notebooks/Scrap_train/saree\"\n",
        "\n",
        "    # iterate through the names of contents of the folder\n",
        "    for image_path in os.listdir(path):\n",
        "\n",
        "        # create the full input path and read the file\n",
        "        input_path = os.path.join(path, image_path)\n",
        "        image_to_sharp = cv2.imread(input_path)\n",
        "\n",
        "        # sharp the image\n",
        "        kernel = np.array([[0, -1, 0], \n",
        "                   [-1, 5,-1], \n",
        "                   [0, -1, 0]])\n",
        "        \n",
        "\n",
        "        # Sharpen image\n",
        "        image_sharp = cv2.filter2D(image_to_sharp, -1, kernel)\n",
        "        \n",
        "        # create full output path, 'example.jpg' \n",
        "        # becomes 'sharp_example.jpg', save the file to disk\n",
        "        fullpath = os.path.join(outPath, 'sharp_'+image_path)\n",
        "        cv2.imwrite(fullpath, image_sharp)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    sharpen()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Y34yYg-NPFn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rotate():\n",
        "    outPath = \"/content/gdrive/My Drive/Colab Notebooks/A4 Augmented images/Saree\"\n",
        "    path = \"/content/gdrive/My Drive/Colab Notebooks/Scrap_train/saree\"\n",
        "\n",
        "    # iterate through the names of contents of the folder\n",
        "    for image_path in os.listdir(path):\n",
        "\n",
        "        # create the full input path and read the file\n",
        "        input_path = os.path.join(path, image_path)\n",
        "        image_to_rotate = cv2.imread(input_path)\n",
        "\n",
        "        # rotate the image\n",
        "        height, width= image_to_rotate.shape[:2]\n",
        "        rotation_matrix=cv2.getRotationMatrix2D((width/2, height/2), 30, 0.5)\n",
        "        rotated=cv2.warpAffine(image_to_rotate, rotation_matrix, (width, height))\n",
        "\n",
        "        fullpath = os.path.join(outPath, 'rotated_'+image_path)\n",
        "        cv2.imwrite(fullpath, rotated)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    rotate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9YdulgmNO08",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rotate():\n",
        "    outPath = \"/content/gdrive/My Drive/Colab Notebooks/A4 Augmented images/Saree\"\n",
        "    path = \"/content/gdrive/My Drive/Colab Notebooks/Scrap_train/saree\"\n",
        "\n",
        "    # iterate through the names of contents of the folder\n",
        "    for image_path in os.listdir(path):\n",
        "\n",
        "        # create the full input path and read the file\n",
        "        input_path = os.path.join(path, image_path)\n",
        "        image_to_rotate = cv2.imread(input_path)\n",
        "\n",
        "        # rotate the image\n",
        "        height, width= image_to_rotate.shape[:2]\n",
        "        rotation_matrix=cv2.getRotationMatrix2D((width/2, height/2), -30, 0.5)\n",
        "        rotated=cv2.warpAffine(image_to_rotate, rotation_matrix, (width, height))\n",
        "\n",
        "        fullpath = os.path.join(outPath, 'rotated_'+image_path)\n",
        "        cv2.imwrite(fullpath, rotated)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    rotate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A94MJ-bCJoel",
        "colab_type": "text"
      },
      "source": [
        "Data Augmentation using opencv (trousers)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qKH1I9wJjfP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rotate():\n",
        "    outPath = \"/content/gdrive/My Drive/Colab Notebooks/A4 Augmented images/trousers\"\n",
        "    path = \"/content/gdrive/My Drive/Colab Notebooks/Scrap_train/trousers\"\n",
        "\n",
        "    # iterate through the names of contents of the folder\n",
        "    for image_path in os.listdir(path):\n",
        "\n",
        "        # create the full input path and read the file\n",
        "        input_path = os.path.join(path, image_path)\n",
        "        image_to_rotate = cv2.imread(input_path)\n",
        "\n",
        "        # rotate the image\n",
        "        img_rotate_90_clockwise = cv2.rotate(image_to_rotate, cv2.ROTATE_90_CLOCKWISE)\n",
        "\n",
        "\n",
        "        # create full output path, 'example.jpg' \n",
        "        # becomes 'rotate_example.jpg', save the file to disk\n",
        "        fullpath = os.path.join(outPath, 'rotated_'+image_path)\n",
        "        cv2.imwrite(fullpath, img_rotate_90_clockwise)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    rotate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMD9bnAkJxGT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resize():\n",
        "    outPath = \"/content/gdrive/My Drive/Colab Notebooks/A4 Augmented images/trousers\"\n",
        "    path = \"/content/gdrive/My Drive/Colab Notebooks/Scrap_train/trousers\"\n",
        "\n",
        "    # iterate through the names of contents of the folder\n",
        "    for image_path in os.listdir(path):\n",
        "\n",
        "        # create the full input path and read the file\n",
        "        input_path = os.path.join(path, image_path)\n",
        "        image_to_resize = cv2.imread(input_path)\n",
        "\n",
        "        # resize the image\n",
        "        image_resized = cv2.resize(image_to_resize, (300, 200))\n",
        "\n",
        "        image_rgb = cv2.cvtColor(image_to_resize, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # create full output path, 'example.jpg' \n",
        "        # becomes 'resize_example.jpg', save the file to disk\n",
        "        fullpath = os.path.join(outPath, 'resize_'+image_path)\n",
        "        cv2.imwrite(fullpath, image_resized)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    resize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSJz4JckJ1K4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def flip():\n",
        "    outPath = \"/content/gdrive/My Drive/Colab Notebooks/A4 Augmented images/trousers\"\n",
        "    path = \"/content/gdrive/My Drive/Colab Notebooks/Scrap_train/trousers\"\n",
        "\n",
        "    # iterate through the names of contents of the folder\n",
        "    for image_path in os.listdir(path):\n",
        "\n",
        "        # create the full input path and read the file\n",
        "        input_path = os.path.join(path, image_path)\n",
        "        image_to_flip = cv2.imread(input_path)\n",
        "\n",
        "        # flip the image\n",
        "        img_flip_ud = cv2.flip(image_to_flip, 0)\n",
        "        \n",
        "\n",
        "\n",
        "        # create full output path, 'example.jpg' \n",
        "        # becomes 'flip_example.jpg', save the file to disk\n",
        "        fullpath = os.path.join(outPath, 'flip_'+image_path)\n",
        "        cv2.imwrite(fullpath, img_flip_ud)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    flip()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixYJy74pJ4Zc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sharpen():\n",
        "    outPath = \"/content/gdrive/My Drive/Colab Notebooks/A4 Augmented images/trousers\"\n",
        "    path = \"/content/gdrive/My Drive/Colab Notebooks/Scrap_train/trousers\"\n",
        "\n",
        "    # iterate through the names of contents of the folder\n",
        "    for image_path in os.listdir(path):\n",
        "\n",
        "        # create the full input path and read the file\n",
        "        input_path = os.path.join(path, image_path)\n",
        "        image_to_sharp = cv2.imread(input_path)\n",
        "\n",
        "        # sharp the image\n",
        "        kernel = np.array([[0, -1, 0], \n",
        "                   [-1, 5,-1], \n",
        "                   [0, -1, 0]])\n",
        "        \n",
        "\n",
        "        # Sharpen image\n",
        "        image_sharp = cv2.filter2D(image_to_sharp, -1, kernel)\n",
        "        \n",
        "        # create full output path, 'example.jpg' \n",
        "        # becomes 'sharp_example.jpg', save the file to disk\n",
        "        fullpath = os.path.join(outPath, 'sharp_'+image_path)\n",
        "        cv2.imwrite(fullpath, image_sharp)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    sharpen()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_OVwNvlJ81W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bright():\n",
        "    outPath = \"/content/gdrive/My Drive/Colab Notebooks/A4 Augmented images/trousers\"\n",
        "    path = \"/content/gdrive/My Drive/Colab Notebooks/Scrap_train/trousers\"\n",
        "\n",
        "    # iterate through the names of contents of the folder\n",
        "    for image_path in os.listdir(path):\n",
        "\n",
        "        # create the full input path and read the file\n",
        "        input_path = os.path.join(path, image_path)\n",
        "        image_to_bright = cv2.imread(input_path)\n",
        "\n",
        "        # bright the image\n",
        "        image= cv2.add(image_to_bright,np.array([50.0]))\n",
        "        \n",
        "\n",
        "\n",
        "        # create full output path, 'example.jpg' \n",
        "        # becomes 'bright_example.jpg', save the file to disk\n",
        "        fullpath = os.path.join(outPath, 'bright_'+image_path)\n",
        "        cv2.imwrite(fullpath, image)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    bright()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcMIlhkUKAQX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decbright():\n",
        "    outPath = \"/content/gdrive/My Drive/Colab Notebooks/A4 Augmented images/trousers\"\n",
        "    path = \"/content/gdrive/My Drive/Colab Notebooks/Scrap_train/trousers\"\n",
        "\n",
        "    # iterate through the names of contents of the folder\n",
        "    for image_path in os.listdir(path):\n",
        "\n",
        "        # create the full input path and read the file\n",
        "        input_path = os.path.join(path, image_path)\n",
        "        image_to_bright = cv2.imread(input_path)\n",
        "\n",
        "        # bright the image\n",
        "        image= cv2.add(image_to_bright,np.array([-50.0]))\n",
        "        \n",
        "\n",
        "\n",
        "        # create full output path, 'example.jpg' \n",
        "        # becomes 'bright_example.jpg', save the file to disk\n",
        "        fullpath = os.path.join(outPath, 'decbright_'+image_path)\n",
        "        cv2.imwrite(fullpath, image)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    decbright()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h--3ZYkvOXuU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rotate():\n",
        "    outPath = \"/content/gdrive/My Drive/Colab Notebooks/A4 Augmented images/trousers\"\n",
        "    path = \"/content/gdrive/My Drive/Colab Notebooks/Scrap_train/trousers\"\n",
        "\n",
        "    # iterate through the names of contents of the folder\n",
        "    for image_path in os.listdir(path):\n",
        "\n",
        "        # create the full input path and read the file\n",
        "        input_path = os.path.join(path, image_path)\n",
        "        image_to_rotate = cv2.imread(input_path)\n",
        "\n",
        "        # rotate the image\n",
        "        height, width= image_to_rotate.shape[:2]\n",
        "        rotation_matrix=cv2.getRotationMatrix2D((width/2, height/2), -30, 0.5)\n",
        "        rotated=cv2.warpAffine(image_to_rotate, rotation_matrix, (width, height))\n",
        "\n",
        "        fullpath = os.path.join(outPath, 'rotated_'+image_path)\n",
        "        cv2.imwrite(fullpath, rotated)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    rotate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjFHsyvMOgVH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rotate():\n",
        "    outPath = \"/content/gdrive/My Drive/Colab Notebooks/A4 Augmented images/trousers\"\n",
        "    path = \"/content/gdrive/My Drive/Colab Notebooks/Scrap_train/trousers\"\n",
        "\n",
        "    # iterate through the names of contents of the folder\n",
        "    for image_path in os.listdir(path):\n",
        "\n",
        "        # create the full input path and read the file\n",
        "        input_path = os.path.join(path, image_path)\n",
        "        image_to_rotate = cv2.imread(input_path)\n",
        "\n",
        "        # rotate the image\n",
        "        height, width= image_to_rotate.shape[:2]\n",
        "        rotation_matrix=cv2.getRotationMatrix2D((width/2, height/2), 30, 0.5)\n",
        "        rotated=cv2.warpAffine(image_to_rotate, rotation_matrix, (width, height))\n",
        "\n",
        "        fullpath = os.path.join(outPath, 'rotated_'+image_path)\n",
        "        cv2.imwrite(fullpath, rotated)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    rotate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJVs7XprKH08",
        "colab_type": "text"
      },
      "source": [
        "BUILDING CONVOLUTIONAL NEURAL NETWORK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBINA1l8KDZI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9861bed8-18e2-448b-e84b-a1f77e50cf2c"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense\n",
        "from keras.preprocessing.image import ImageDataGenerator, load_img\n",
        "from keras import backend as K"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoMhnpsKMIe-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "88dd874d-035e-4123-b6d0-7ecae5c0610b"
      },
      "source": [
        "# dimensions of our images.\n",
        "#input_shape=(256,256,3)\n",
        "img_width, img_height = 150, 150\n",
        "\n",
        "\n",
        "trdata = ImageDataGenerator()\n",
        "traindata = trdata.flow_from_directory(directory=\"/content/gdrive/My Drive/Colab Notebooks/A4 Augmented images\")\n",
        "tsdata = ImageDataGenerator()\n",
        "testdata = tsdata.flow_from_directory(directory=\"/content/gdrive/My Drive/Colab Notebooks/Scrap_test\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1842 images belonging to 3 classes.\n",
            "Found 308 images belonging to 3 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwWfVS9TMSuM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "nb_train_samples=1842\n",
        "nb_validation_samples=308\n",
        "batch_size=32\n",
        "epochs=20"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHStfs96NwpT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "b507c9f7-00c2-41ff-884c-4b9a1fc175a9"
      },
      "source": [
        "#model building\n",
        "model = Sequential()\n",
        "model.add(Conv2D(input_shape=(256,256,3),filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "model.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "#flatten layer\n",
        "model.add(Flatten()) # Output convert into one dimension layer and will go to Dense layer\n",
        "model.add(Dense(64))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1))\n",
        "model.add(Dense(3, activation='softmax'))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1gSvA4TODmO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8f416975-e3b2-4008-dc49-dac967968df2"
      },
      "source": [
        "model.summary()\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 256, 256, 64)      1792      \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 256, 256, 64)      36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 128, 128, 64)      0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128, 128, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 128, 128, 128)     73856     \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 128, 128, 128)     147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 64, 64, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 64, 64, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 64, 64, 256)       295168    \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 64, 64, 256)       590080    \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 64, 64, 256)       590080    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 32, 32, 256)       0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 32, 32, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 32, 32, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 32, 32, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 32, 32, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 16, 16, 512)       0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 16, 16, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 16, 16, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 16, 16, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 16, 16, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 8, 8, 512)         0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 8, 8, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 32768)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                2097216   \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 65        \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 3)                 6         \n",
            "=================================================================\n",
            "Total params: 16,811,975\n",
            "Trainable params: 16,811,975\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIL2UY32jbgy",
        "colab_type": "text"
      },
      "source": [
        "SGD OPTIMIZER\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDtVe-RfOE3Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import Callback\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "checkpoint= ModelCheckpoint(\"Modelsgd.h5\", monitor=\"val_accuracy\",mode='max', save_best_only=True,verbose=1)\n",
        "\n",
        "earlystop= EarlyStopping(monitor=\"val_accuracy\",min_delta=0,patience=5,verbose=1,restore_best_weights=True)\n",
        "\n",
        "reduce_lr= ReduceLROnPlateau(monitor=\"val_accuracy\", factor=0.2, patience=5, verbose=1, min_delta=0.001)\n",
        "\n",
        "#putiing callbacks into callback list\n",
        "callbacks = [earlystop, checkpoint, reduce_lr]\n",
        "momentums = [0.0, 0.5, 0.9, 0.99]\n",
        "opt = SGD(lr=0.01, momentum=0.5, decay=0.01)\n",
        "#we use a very small learning rate\n",
        "model.compile(loss = 'categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHOko6gdVyw4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9d92d1fd-a6c4-4bc3-f312-d7a8330eac19"
      },
      "source": [
        "history=model.fit_generator(\n",
        "    traindata,\n",
        "    steps_per_epoch=nb_train_samples//batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=testdata,\n",
        "    validation_steps=nb_validation_samples//batch_size,\n",
        "    callbacks = [earlystop, checkpoint, reduce_lr])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "Epoch 1/20\n",
            "57/57 [==============================] - 533s 9s/step - loss: 1.1588 - accuracy: 0.4039 - val_loss: 1.0858 - val_accuracy: 0.3403\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.34028, saving model to Modelsgd.h5\n",
            "Epoch 2/20\n",
            "57/57 [==============================] - 56s 989ms/step - loss: 1.0416 - accuracy: 0.5453 - val_loss: 1.2472 - val_accuracy: 0.3587\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.34028 to 0.35870, saving model to Modelsgd.h5\n",
            "Epoch 3/20\n",
            "57/57 [==============================] - 51s 897ms/step - loss: 0.9817 - accuracy: 0.5287 - val_loss: 0.3769 - val_accuracy: 0.6739\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.35870 to 0.67391, saving model to Modelsgd.h5\n",
            "Epoch 4/20\n",
            "57/57 [==============================] - 51s 898ms/step - loss: 0.6485 - accuracy: 0.6878 - val_loss: 0.3274 - val_accuracy: 0.9058\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.67391 to 0.90580, saving model to Modelsgd.h5\n",
            "Epoch 5/20\n",
            "57/57 [==============================] - 51s 898ms/step - loss: 0.4085 - accuracy: 0.8298 - val_loss: 0.3456 - val_accuracy: 0.9203\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.90580 to 0.92029, saving model to Modelsgd.h5\n",
            "Epoch 6/20\n",
            "57/57 [==============================] - 51s 903ms/step - loss: 0.3272 - accuracy: 0.8690 - val_loss: 0.5046 - val_accuracy: 0.8732\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.92029\n",
            "Epoch 7/20\n",
            "57/57 [==============================] - 51s 894ms/step - loss: 0.2772 - accuracy: 0.9037 - val_loss: 0.1789 - val_accuracy: 0.9384\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.92029 to 0.93841, saving model to Modelsgd.h5\n",
            "Epoch 8/20\n",
            "57/57 [==============================] - 51s 900ms/step - loss: 0.1917 - accuracy: 0.9381 - val_loss: 0.0485 - val_accuracy: 0.9638\n",
            "\n",
            "Epoch 00008: val_accuracy improved from 0.93841 to 0.96377, saving model to Modelsgd.h5\n",
            "Epoch 9/20\n",
            "57/57 [==============================] - 51s 903ms/step - loss: 0.1823 - accuracy: 0.9375 - val_loss: 0.1136 - val_accuracy: 0.9674\n",
            "\n",
            "Epoch 00009: val_accuracy improved from 0.96377 to 0.96739, saving model to Modelsgd.h5\n",
            "Epoch 10/20\n",
            "57/57 [==============================] - 51s 896ms/step - loss: 0.1460 - accuracy: 0.9488 - val_loss: 0.0265 - val_accuracy: 0.9855\n",
            "\n",
            "Epoch 00010: val_accuracy improved from 0.96739 to 0.98551, saving model to Modelsgd.h5\n",
            "Epoch 11/20\n",
            "57/57 [==============================] - 51s 901ms/step - loss: 0.1162 - accuracy: 0.9613 - val_loss: 0.0383 - val_accuracy: 0.9861\n",
            "\n",
            "Epoch 00011: val_accuracy improved from 0.98551 to 0.98611, saving model to Modelsgd.h5\n",
            "Epoch 12/20\n",
            "57/57 [==============================] - 51s 900ms/step - loss: 0.1041 - accuracy: 0.9619 - val_loss: 0.0117 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00012: val_accuracy improved from 0.98611 to 1.00000, saving model to Modelsgd.h5\n",
            "Epoch 13/20\n",
            "57/57 [==============================] - 51s 900ms/step - loss: 0.0781 - accuracy: 0.9713 - val_loss: 0.0127 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 1.00000\n",
            "Epoch 14/20\n",
            "57/57 [==============================] - 51s 898ms/step - loss: 0.0869 - accuracy: 0.9729 - val_loss: 0.0138 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 1.00000\n",
            "Epoch 15/20\n",
            "57/57 [==============================] - 51s 899ms/step - loss: 0.0592 - accuracy: 0.9818 - val_loss: 0.0165 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 1.00000\n",
            "Epoch 16/20\n",
            "57/57 [==============================] - 51s 898ms/step - loss: 0.0597 - accuracy: 0.9796 - val_loss: 0.0054 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 1.00000\n",
            "Epoch 17/20\n",
            "57/57 [==============================] - 51s 897ms/step - loss: 0.0480 - accuracy: 0.9851 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
            "Restoring model weights from the end of the best epoch\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 1.00000\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
            "Epoch 00017: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3ObU7kpW7ud",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1icBjVVek4cW",
        "colab_type": "text"
      },
      "source": [
        "ADAM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtTtSsMHk6o5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import Callback\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "checkpoint= ModelCheckpoint(\"Modeladam.h5\", monitor=\"val_accuracy\",mode='max', save_best_only=True,verbose=1)\n",
        "\n",
        "earlystop= EarlyStopping(monitor=\"val_accuracy\",min_delta=0,patience=5,verbose=1,restore_best_weights=True)\n",
        "\n",
        "reduce_lr= ReduceLROnPlateau(monitor=\"val_accuracy\", factor=0.2, patience=5, verbose=1, min_delta=0.001)\n",
        "\n",
        "#putiing callbacks into callback list\n",
        "callbacks = [earlystop, checkpoint, reduce_lr]\n",
        "opt = Adam(lr=0.01, decay=0.01)\n",
        "#we use a very small learning rate\n",
        "model.compile(loss = 'categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cudfVd2elEHq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "c0fa60b5-b014-47cc-c579-926f203afebc"
      },
      "source": [
        "history=model.fit_generator(\n",
        "    traindata,\n",
        "    steps_per_epoch=nb_train_samples//batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=testdata,\n",
        "    validation_steps=nb_validation_samples//batch_size,\n",
        "    callbacks = [earlystop, checkpoint, reduce_lr])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "57/57 [==============================] - 56s 977ms/step - loss: 50219267918.2038 - accuracy: 0.3481 - val_loss: 1.1013 - val_accuracy: 0.3507\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.35069, saving model to Modeladam.h5\n",
            "Epoch 2/20\n",
            "57/57 [==============================] - 54s 952ms/step - loss: 1.1050 - accuracy: 0.3387 - val_loss: 1.0875 - val_accuracy: 0.3623\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.35069 to 0.36232, saving model to Modeladam.h5\n",
            "Epoch 3/20\n",
            "57/57 [==============================] - 54s 954ms/step - loss: 1.1036 - accuracy: 0.3265 - val_loss: 1.0897 - val_accuracy: 0.3478\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.36232\n",
            "Epoch 4/20\n",
            "57/57 [==============================] - 54s 952ms/step - loss: 1.1051 - accuracy: 0.3282 - val_loss: 1.1549 - val_accuracy: 0.3152\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.36232\n",
            "Epoch 5/20\n",
            "57/57 [==============================] - 54s 951ms/step - loss: 1.1037 - accuracy: 0.3188 - val_loss: 1.0953 - val_accuracy: 0.3623\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.36232\n",
            "Epoch 6/20\n",
            "57/57 [==============================] - 54s 951ms/step - loss: 1.0991 - accuracy: 0.3492 - val_loss: 1.1097 - val_accuracy: 0.3007\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.36232\n",
            "Epoch 7/20\n",
            "57/57 [==============================] - 54s 952ms/step - loss: 1.1023 - accuracy: 0.3215 - val_loss: 1.0986 - val_accuracy: 0.3406\n",
            "Restoring model weights from the end of the best epoch\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.36232\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
            "Epoch 00007: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVR3o0RPl_dO",
        "colab_type": "text"
      },
      "source": [
        "NADAM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iL1YRfOmAwn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import Callback\n",
        "from keras.optimizers import Nadam\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "checkpoint= ModelCheckpoint(\"Modelnadam.h5\", monitor=\"val_accuracy\",mode='max', save_best_only=True,verbose=1)\n",
        "\n",
        "earlystop= EarlyStopping(monitor=\"val_accuracy\",min_delta=0,patience=5,verbose=1,restore_best_weights=True)\n",
        "\n",
        "reduce_lr= ReduceLROnPlateau(monitor=\"val_accuracy\", factor=0.2, patience=5, verbose=1, min_delta=0.001)\n",
        "\n",
        "#putiing callbacks into callback list\n",
        "callbacks = [earlystop, checkpoint, reduce_lr]\n",
        "opt = Nadam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
        "#we use a very small learning rate\n",
        "model.compile(loss = 'categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_4mtriElNR8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "58325119-c768-46f3-feaf-9429d28bdc9c"
      },
      "source": [
        "history=model.fit_generator(\n",
        "    traindata,\n",
        "    steps_per_epoch=nb_train_samples//batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=testdata,\n",
        "    validation_steps=nb_validation_samples//batch_size,\n",
        "    callbacks = [earlystop, checkpoint, reduce_lr])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "57/57 [==============================] - 60s 1s/step - loss: 1.1060 - accuracy: 0.3331 - val_loss: 1.1221 - val_accuracy: 0.3125\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.31250, saving model to Modelnadam.h5\n",
            "Epoch 2/20\n",
            "57/57 [==============================] - 58s 1s/step - loss: 1.1045 - accuracy: 0.3243 - val_loss: 1.0975 - val_accuracy: 0.3587\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.31250 to 0.35870, saving model to Modelnadam.h5\n",
            "Epoch 3/20\n",
            "57/57 [==============================] - 58s 1s/step - loss: 1.1051 - accuracy: 0.3144 - val_loss: 1.0976 - val_accuracy: 0.3587\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.35870\n",
            "Epoch 4/20\n",
            "57/57 [==============================] - 58s 1s/step - loss: 1.1014 - accuracy: 0.3381 - val_loss: 1.1030 - val_accuracy: 0.3261\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.35870\n",
            "Epoch 5/20\n",
            "57/57 [==============================] - 58s 1s/step - loss: 1.1002 - accuracy: 0.3354 - val_loss: 1.1245 - val_accuracy: 0.2717\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.35870\n",
            "Epoch 6/20\n",
            "57/57 [==============================] - 58s 1s/step - loss: 1.1007 - accuracy: 0.3337 - val_loss: 1.1036 - val_accuracy: 0.3478\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.35870\n",
            "Epoch 7/20\n",
            "57/57 [==============================] - 58s 1s/step - loss: 1.1001 - accuracy: 0.3519 - val_loss: 1.0975 - val_accuracy: 0.3116\n",
            "Restoring model weights from the end of the best epoch\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.35870\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "Epoch 00007: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gm1tB3eZmbQA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHq2S4g2qo-K",
        "colab_type": "text"
      },
      "source": [
        "RMSprop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRYMGYSEqiwZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import Callback\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "checkpoint= ModelCheckpoint(\"Modelrmsprop.h5\", monitor=\"val_accuracy\",mode='max', save_best_only=True,verbose=1)\n",
        "\n",
        "earlystop= EarlyStopping(monitor=\"val_accuracy\",min_delta=0,patience=5,verbose=1,restore_best_weights=True)\n",
        "\n",
        "reduce_lr= ReduceLROnPlateau(monitor=\"val_accuracy\", factor=0.2, patience=5, verbose=1, min_delta=0.001)\n",
        "\n",
        "#putiing callbacks into callback list\n",
        "callbacks = [earlystop, checkpoint, reduce_lr]\n",
        "momentums = [0.0, 0.5, 0.9, 0.99]\n",
        "opt = RMSprop(lr=0.01)\n",
        "#we use a very small learning rate\n",
        "model.compile(loss = 'categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M66zohBArbbv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "outputId": "96e12a61-aa7b-4a89-abe0-3f0db44e9aa6"
      },
      "source": [
        "history=model.fit_generator(\n",
        "    traindata,\n",
        "    steps_per_epoch=nb_train_samples//batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=testdata,\n",
        "    validation_steps=nb_validation_samples//batch_size,\n",
        "    callbacks = [earlystop, checkpoint, reduce_lr])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "57/57 [==============================] - 59s 1s/step - loss: 1.1688 - accuracy: 0.3448 - val_loss: 1.1059 - val_accuracy: 0.3403\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.34028, saving model to Modelrmsprop.h5\n",
            "Epoch 2/20\n",
            "57/57 [==============================] - 59s 1s/step - loss: 1.0990 - accuracy: 0.3492 - val_loss: 1.1033 - val_accuracy: 0.3152\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.34028\n",
            "Epoch 3/20\n",
            "57/57 [==============================] - 58s 1s/step - loss: 1.1003 - accuracy: 0.3377 - val_loss: 1.0830 - val_accuracy: 0.3442\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.34028 to 0.34420, saving model to Modelrmsprop.h5\n",
            "Epoch 4/20\n",
            "57/57 [==============================] - 58s 1s/step - loss: 1.0997 - accuracy: 0.3396 - val_loss: 1.1104 - val_accuracy: 0.3333\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.34420\n",
            "Epoch 5/20\n",
            "57/57 [==============================] - 59s 1s/step - loss: 1.0989 - accuracy: 0.3442 - val_loss: 1.1064 - val_accuracy: 0.3659\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.34420 to 0.36594, saving model to Modelrmsprop.h5\n",
            "Epoch 6/20\n",
            "57/57 [==============================] - 58s 1s/step - loss: 1.0992 - accuracy: 0.3465 - val_loss: 1.0996 - val_accuracy: 0.3080\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.36594\n",
            "Epoch 7/20\n",
            "57/57 [==============================] - 58s 1s/step - loss: 1.0997 - accuracy: 0.3291 - val_loss: 1.0991 - val_accuracy: 0.3623\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.36594\n",
            "Epoch 8/20\n",
            "57/57 [==============================] - 60s 1s/step - loss: 1.0994 - accuracy: 0.3348 - val_loss: 1.0951 - val_accuracy: 0.3442\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.36594\n",
            "Epoch 9/20\n",
            "57/57 [==============================] - 58s 1s/step - loss: 1.0992 - accuracy: 0.3138 - val_loss: 1.1126 - val_accuracy: 0.3333\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.36594\n",
            "Epoch 10/20\n",
            "57/57 [==============================] - 60s 1s/step - loss: 1.0983 - accuracy: 0.3586 - val_loss: 1.1128 - val_accuracy: 0.3551\n",
            "Restoring model weights from the end of the best epoch\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.36594\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
            "Epoch 00010: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qy9rGz78sQ84",
        "colab_type": "text"
      },
      "source": [
        " Nesterov Accelerated Gradient Descent\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1e76eoofrhrs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import Callback\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "checkpoint= ModelCheckpoint(\"ModelNag.h5\", monitor=\"val_accuracy\",mode='max', save_best_only=True,verbose=1)\n",
        "\n",
        "earlystop= EarlyStopping(monitor=\"val_accuracy\",min_delta=0,patience=5,verbose=1,restore_best_weights=True)\n",
        "\n",
        "reduce_lr= ReduceLROnPlateau(monitor=\"val_accuracy\", factor=0.2, patience=5, verbose=1, min_delta=0.001)\n",
        "\n",
        "#putiing callbacks into callback list\n",
        "callbacks = [earlystop, checkpoint, reduce_lr]\n",
        "opt = SGD(lr=0.01, nesterov=True)\n",
        "#we use a very small learning rate\n",
        "model.compile(loss = 'categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgYj_oqVsYcr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955
        },
        "outputId": "f7ed7e41-3940-4d40-83dd-16c77fbf1894"
      },
      "source": [
        "history=model.fit_generator(\n",
        "    traindata,\n",
        "    steps_per_epoch=nb_train_samples//batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=testdata,\n",
        "    validation_steps=nb_validation_samples//batch_size,\n",
        "    callbacks = [earlystop, checkpoint, reduce_lr])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "Epoch 1/20\n",
            "57/57 [==============================] - 1284s 23s/step - loss: 1.5978 - accuracy: 0.3939 - val_loss: 1.0820 - val_accuracy: 0.4062\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.40625, saving model to ModelNag.h5\n",
            "Epoch 2/20\n",
            "57/57 [==============================] - 83s 1s/step - loss: 1.0609 - accuracy: 0.4376 - val_loss: 1.0569 - val_accuracy: 0.6739\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.40625 to 0.67391, saving model to ModelNag.h5\n",
            "Epoch 3/20\n",
            "57/57 [==============================] - 64s 1s/step - loss: 1.0144 - accuracy: 0.4989 - val_loss: 1.0434 - val_accuracy: 0.6739\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.67391\n",
            "Epoch 4/20\n",
            "57/57 [==============================] - 64s 1s/step - loss: 0.9019 - accuracy: 0.5558 - val_loss: 0.9011 - val_accuracy: 0.6993\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.67391 to 0.69928, saving model to ModelNag.h5\n",
            "Epoch 5/20\n",
            "57/57 [==============================] - 65s 1s/step - loss: 0.8305 - accuracy: 0.5872 - val_loss: 0.7552 - val_accuracy: 0.6957\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.69928\n",
            "Epoch 6/20\n",
            "57/57 [==============================] - 64s 1s/step - loss: 0.7520 - accuracy: 0.6186 - val_loss: 0.7296 - val_accuracy: 0.6739\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.69928\n",
            "Epoch 7/20\n",
            "57/57 [==============================] - 64s 1s/step - loss: 0.6801 - accuracy: 0.6320 - val_loss: 0.5780 - val_accuracy: 0.7065\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.69928 to 0.70652, saving model to ModelNag.h5\n",
            "Epoch 8/20\n",
            "57/57 [==============================] - 64s 1s/step - loss: 0.6528 - accuracy: 0.6480 - val_loss: 0.6640 - val_accuracy: 0.6486\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.70652\n",
            "Epoch 9/20\n",
            "57/57 [==============================] - 64s 1s/step - loss: 0.6041 - accuracy: 0.6470 - val_loss: 0.6146 - val_accuracy: 0.6703\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.70652\n",
            "Epoch 10/20\n",
            "57/57 [==============================] - 64s 1s/step - loss: 0.5963 - accuracy: 0.6624 - val_loss: 0.4867 - val_accuracy: 0.6558\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.70652\n",
            "Epoch 11/20\n",
            "57/57 [==============================] - 64s 1s/step - loss: 0.5471 - accuracy: 0.6746 - val_loss: 0.4933 - val_accuracy: 0.6840\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.70652\n",
            "Epoch 12/20\n",
            "57/57 [==============================] - 64s 1s/step - loss: 0.5621 - accuracy: 0.6613 - val_loss: 0.5136 - val_accuracy: 0.6413\n",
            "Restoring model weights from the end of the best epoch\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.70652\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
            "Epoch 00012: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Zd2NapnsfJy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}